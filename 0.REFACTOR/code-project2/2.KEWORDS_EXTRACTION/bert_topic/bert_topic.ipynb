{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gozde\\.conda\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "import datetime\n",
    "import os \n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_now_str():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_strings_to_arrays(df, col_names = ['tokenized_sentences', 'tokenized_words', 'tokenized_words_processed', 'tokenized_words_no_stopwords', 'tokenized_words_lemmatize']):\n",
    "    for col in col_names:\n",
    "        df[col] = df[col].apply(eval)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_bertopic(data_path, models_path, results_path, timestamp, min_topic_size=6, top_n_words=22):\n",
    "    \"\"\"Performs bertopic keywords extraction for data after lemmatization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to preprocessed dataset. Dataset must contain a column with name 'tokenized_words_lemmatize'.\n",
    "    \n",
    "    models_path : str\n",
    "        Path to save the model to (folder must exist).\n",
    "\n",
    "    results_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    timestamp : str\n",
    "        timestamp that will be added to filenames\n",
    "\n",
    "    min_topic_size: int\n",
    "        minimal number of datapoints in topic\n",
    "\n",
    "    top_n_words: int\n",
    "        number of extracted keywords\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    (result_path, model_save_name) : tuple[str]\n",
    "        Frist element is the path to created file with extracted keywrods, second - path to created model.\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    from bertopic import BERTopic\n",
    "    import os \n",
    "    import numpy as np\n",
    "\n",
    "    # basic BertTopic keyword extraction\n",
    "    def train_transform_save(train_data, model_save_name, min_topic_size=10):\n",
    "        \n",
    "        # train transform\n",
    "        topic_model = BERTopic(min_topic_size=min_topic_size, top_n_words=top_n_words)\n",
    "        topics, probs = topic_model.fit_transform(train_data.values)\n",
    "\n",
    "        # save model\n",
    "        topic_model.save(model_save_name)\n",
    "\n",
    "        return topic_model, topics, probs\n",
    "\n",
    "\n",
    "    def load_transform_save(data, model_save_name, results_path):\n",
    "\n",
    "        # load model\n",
    "        loaded_model = BERTopic.load(model_save_name)\n",
    "\n",
    "        # transform for data \n",
    "        samples_topics, samples_probs = loaded_model.transform(data.values)\n",
    "        res_df = pd.DataFrame({\n",
    "            'PMID': np.unique(data.index),\n",
    "            'topic_number': samples_topics,\n",
    "            'topic_probs': samples_probs,\n",
    "            \"topic_keywords\": [loaded_model.get_topic(topic_number) for topic_number in samples_topics]\n",
    "        })\n",
    "        res_df.to_csv(results_path, index=False)\n",
    "        return loaded_model, res_df\n",
    "\n",
    "    ##############################################################################################################################\n",
    "\n",
    "    full_data = transform_strings_to_arrays(pd.read_csv(data_path), col_names=['tokenized_words_lemmatize'])\n",
    "\n",
    "    data = full_data.groupby(by = ['PMID'])['tokenized_words_lemmatize'].agg(lambda x: ' '.join(x.values[0] + x.values[1]))\n",
    "\n",
    "    model_name = f'bertopic_keywords_{timestamp}'\n",
    "    model_save_name = os.path.join(models_path, model_name)\n",
    "    result_path = os.path.join(results_path, 'bertopic', f'{model_name}.csv')\n",
    "\n",
    "    topic_model, topics, probs = train_transform_save(data, model_save_name, min_topic_size=3)\n",
    "    _, res_df = load_transform_save(data, model_save_name, result_path)\n",
    "\n",
    "    return result_path, model_save_name "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic for CRAFT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../0.RESULTS/bertopic\\\\bertopic_keywords_2023-01-23_11-11-41.csv',\n",
       " '../../0.RESULTS/bertopic/models/bertopic_keywords_2023-01-23_11-11-41')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../../0.RESULTS/preprocessing/data_whole.csv\"\n",
    "models_path = \"../../0.RESULTS/bertopic/models/\"\n",
    "results_path = \"../../0.RESULTS/\"\n",
    "timestamp = get_now_str()\n",
    "get_keywords_bertopic(data_path, models_path, results_path, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic.load('../../0.RESULTS/bertopic/models/bertopic_keywords_2023-01-23_11-11-41')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>24</td>\n",
       "      <td>-1_gene_cell_expression_mouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0_cell_mouse_embryonic_mutant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1_mouse_strain_muscle_background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2_mouse_protein_pax6_differentiation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3_annexin_a7_protein_ranbp2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4_olfactory_receptor_mouse_sox1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5_pulmonary_development_individual_lung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6_ear_sensory_cell_hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7_bone_bmp2_bmp4_limb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                     Name\n",
       "0     -1     24            -1_gene_cell_expression_mouse\n",
       "1      0     16            0_cell_mouse_embryonic_mutant\n",
       "2      1     15         1_mouse_strain_muscle_background\n",
       "3      2     13     2_mouse_protein_pax6_differentiation\n",
       "4      3      8              3_annexin_a7_protein_ranbp2\n",
       "5      4      7          4_olfactory_receptor_mouse_sox1\n",
       "6      5      6  5_pulmonary_development_individual_lung\n",
       "7      6      5                  6_ear_sensory_cell_hair\n",
       "8      7      3                    7_bone_bmp2_bmp4_limb"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envNNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69b1adaa11ceff177c5ff5a0e22271ae8b2837f309a3421a8d0197a8c1aada63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
