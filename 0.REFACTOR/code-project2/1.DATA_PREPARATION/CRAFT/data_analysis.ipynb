{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting articles text\n",
    "- to merge abstarct and title one needs to concat them with new line\n",
    "- 3 possible cases of abstarcts (1. Abstarct and text of it, 2. Abstract divided into section 3. Diffrent division for the one document (14611657))\n",
    "- Maxline needed to get only relevant annoatations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_folder = os.path.join('articles','txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line is a title. Then we wait for the abstract. The text of the abstarct is everything until the empty line occure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_whole = pd.DataFrame(columns = ['PMID','Type','Content','MaxLine'])\n",
    "s = []\n",
    "for file in os.listdir(articles_folder):\n",
    "    if file[-3:] == 'txt':\n",
    "        name = file[:-4]\n",
    "        with open(os.path.join(articles_folder,file),'r',encoding='utf-8') as f:\n",
    "            lines = list(f.readlines())\n",
    "            texts_whole.loc[len(texts_whole)] = {'PMID':name, 'Type':'t','Content':lines[0]}\n",
    "            texts_whole.loc[len(texts_whole)] = {'PMID':name, 'Type':'a','Content':''.join(lines[1:]),'MaxLine':len(''.join(lines))}\n",
    "                        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.DataFrame(columns = ['PMID','Type','Content','MaxLine'])\n",
    "s = []\n",
    "for file in os.listdir(articles_folder):\n",
    "    if file[-3:] == 'txt':\n",
    "        name = file[:-4]\n",
    "        with open(os.path.join(articles_folder,file),'r',encoding='utf-8') as f:\n",
    "            article  = pd.DataFrame()\n",
    "            lines = list(f.readlines())\n",
    "            texts.loc[len(texts)] = {'PMID':name, 'Type':'t','Content':lines[0]}\n",
    "            if lines[4] not in [ 'Background\\n','Backgroud\\n','Introduction\\n'] and name != '14611657':\n",
    "                texts.loc[len(texts)] = {'PMID':name, 'Type':'a','Content':''.join(lines[1:5]), 'MaxLine':len(''.join(lines[:5]))}\n",
    "            else:\n",
    "                if name == '14611657':\n",
    "                    txt = ''.join(lines[1:7])\n",
    "                    for i,line in enumerate(lines[7:]):\n",
    "                        if line == 'Background\\n' or line == 'Introduction\\n':\n",
    "                            break\n",
    "                        txt = ''.join([txt,line])\n",
    "                    texts.loc[len(texts)] = {'PMID':name, 'Type':'a','Content':txt,'MaxLine':len(''.join(lines[:(7+i)]))}\n",
    "                else:\n",
    "                    txt = ''.join(lines[1:5])\n",
    "                    for i,line in enumerate(lines[5:]):\n",
    "                        if line == 'Background\\n' or line == 'Introduction\\n':\n",
    "                            break\n",
    "                        txt = ''.join([txt,line])\n",
    "                    texts.loc[len(texts)] = {'PMID':name, 'Type':'a','Content':txt,'MaxLine':len(''.join(lines[:(5+i)]))}\n",
    "                        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.to_csv(os.path.join('data', 'data.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontologies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document has two types of tags\n",
    "1. annotation - information about annotations found in the document \n",
    "    Each annoatation has 4 tags:\n",
    "    a. mention ( from this we get the id of the mention)\n",
    "    b. span - we get the beginning and the end of the annoation\n",
    "    c. annotator - info about annoatator (not used)\n",
    "    d. spannedText - not used\n",
    "2. classMention - information about the class tagged \n",
    "    id - the id of the annoatation coresponding to 1.mention\n",
    "    mentionClass - has info about class id from the ontology and text of the annoatation\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTATNT - the text data need to be in variable texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_for_ontology(ontology):\n",
    "    folder = os.path.join('concept-annotation',ontology,ontology,'knowtator')\n",
    "    return folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(root):\n",
    "    annotations = pd.DataFrame(columns=['StartIndex', 'EndIndex','MentionTextSegment','EntityID'])\n",
    "    for child in root:\n",
    "    # annotation\n",
    "        if child.tag=='annotation':\n",
    "            tmp = {}\n",
    "            id_name = None\n",
    "            for c in child:\n",
    "                if c.tag == 'mention':\n",
    "                    id_name = c.attrib['id']\n",
    "                elif c.tag == 'span':\n",
    "                    tmp['StartIndex'] = c.attrib['start']\n",
    "                    tmp['EndIndex'] = c.attrib['end']\n",
    "            annotations.loc[id_name,['StartIndex','EndIndex']] = tmp\n",
    "\n",
    "    # classmention\n",
    "        else:\n",
    "            id_name =child.attrib['id']\n",
    "            tmp = {}\n",
    "            for c in child:\n",
    "                if c.tag == 'mentionClass' and 'id' in c.attrib.keys():\n",
    "                    tmp['MentionTextSegment'] = c.text\n",
    "                    tmp['EntityID'] = c.attrib['id']\n",
    "                    annotations.loc[id_name,['MentionTextSegment','EntityID']] = tmp\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_before_max(data,max_line):\n",
    "    # get records where StartIndex not empty and smaller than max_line\n",
    "    data = data[~pd.isna(data['StartIndex'])]\n",
    "    return data[data['StartIndex'].astype(int) < max_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(ontology,file_name):\n",
    "    folder = get_folder_for_ontology(ontology)\n",
    "    data = pd.DataFrame()\n",
    "    for file in os.listdir(folder):\n",
    "        tree = ET.parse(os.path.join(folder,file))\n",
    "        root = tree.getroot()\n",
    "        annotations = get_data_from_file(root)\n",
    "        annotations['PMID'] = file[:8]\n",
    "        data = pd.concat([data,annotations])\n",
    "    file_path = os.path.join('data',ontology,file_name)\n",
    "    isExist = os.path.exists(os.path.join('data',ontology))\n",
    "    if not isExist:\n",
    "        os.makedirs(os.path.join('data',ontology))\n",
    "    max_lines = list(texts[texts.PMID == file[:8]].MaxLine)[1]\n",
    "    data = get_before_max(data,max_lines)\n",
    "    data.to_csv(file_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ontology in os.listdir('concept-annotation'):\n",
    "    if ontology in [ 'README.md', 'MONDO']:\n",
    "        continue\n",
    "    get_data(ontology,'annotations.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Complex trait analysis of the mouse striatum: ...\n",
       "1      \\nAbstract\\n\\nBackground\\n\\nThe striatum plays...\n",
       "2      Intraocular pressure in genetically distinct m...\n",
       "3      \\nAbstract\\n\\nBackground\\n\\nLittle is known ab...\n",
       "4                   BRCA2 and homologous recombination\\n\n",
       "                             ...                        \n",
       "189    \\nAbstract\\n\\nIt has long been known that loss...\n",
       "190    Complex Seizure Disorder Caused by Brunol4 Def...\n",
       "191    \\nAbstract\\n\\nIdiopathic epilepsy is a common ...\n",
       "192    Mouse Pachytene Checkpoint 2 (Trip13) Is Requi...\n",
       "193    \\nAbstract\\n\\nIn mammalian meiosis, homologous...\n",
       "Name: Content, Length: 194, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_whole['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts['tokenized_sentences'] = texts['Content'].apply(lambda text : nltk.sent_tokenize(text))\n",
    "texts_whole['tokenized_sentences'] = texts_whole['Content'].apply(lambda text : nltk.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## words\n",
    "```\n",
    "tokenized_words - divide each sentence into words - result format list of lists\n",
    "tokenized_words_processed - divide each sentence into words, delete stopwords and perform stemming\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sentence: list) -> list:\n",
    "    \"\"\"\n",
    "    takes list of tokens and returns steamed tokens without stopwords\n",
    "    If word contains non-letters it appends it to the final list\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            word_lower = word.lower() \n",
    "            if word_lower not in stopwords.words():\n",
    "                processed.append(ps.stem(word))\n",
    "        except TypeError: # when word contains non-letters\n",
    "            processed.append(word)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lemma(sentence: list) -> list:\n",
    "    \"\"\"\n",
    "    takes list of tokens and returns steamed tokens without stopwords\n",
    "    If word contains non-letters it appends it to the final list\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            word_lower = word.lower() \n",
    "            if word_lower not in stopwords.words():\n",
    "                processed.append(lemmatizer.lemmatize(word))\n",
    "        except TypeError: # when word contains non-letters\n",
    "            processed.append(word)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts_whole['tokenized_words'] = None\n",
    "texts_whole['tokenized_words_processed'] = None\n",
    "texts_whole['tokenized_words_lemmatize'] = None\n",
    "for index, row in tqdm(texts_whole.iterrows(), total = len(texts_whole)):\n",
    "    tokens = []\n",
    "    tokens_processed= []\n",
    "    tokens_lemma = []\n",
    "    for sentence in row['tokenized_sentences']:\n",
    "        tok_sen = nltk.word_tokenize(sentence)\n",
    "        tokens.append(tok_sen)\n",
    "        tokens_processed.append(process(tok_sen))\n",
    "        tokens_lemma.append(process_lemma(tok_sen))\n",
    "    texts_whole.at[index,'tokenized_words']  = tokens\n",
    "    texts_whole.at[index,'tokenized_words_processed']  = tokens_processed\n",
    "    texts_whole.at[index, 'tokenized_words_lemmatize'] = tokens_lemma\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts['tokenized_words'] = None\n",
    "texts['tokenized_words_processed'] = None\n",
    "texts['tokenized_words_lemmatize'] = None\n",
    "for index, row in tqdm(texts.iterrows(), total = len(texts)):\n",
    "    tokens = []\n",
    "    tokens_processed= []\n",
    "    tokens_lemma = []\n",
    "    for sentence in row['tokenized_sentences']:\n",
    "        tok_sen = nltk.word_tokenize(sentence)\n",
    "        tokens.append(tok_sen)\n",
    "        tokens_processed.append(process(tok_sen))\n",
    "        tokens_lemma.append(process_lemma(tok_sen))\n",
    "    texts.at[index,'tokenized_words']  = tokens\n",
    "    texts.at[index,'tokenized_words_processed']  = tokens_processed\n",
    "    texts.at[index, 'tokenized_words_lemmatize'] = tokens_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "delete non alpha numeric words\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:00<00:00, 4878.21it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in tqdm(texts.iterrows(), total = len(texts)):\n",
    "    tokens = []\n",
    "    for sentence in row['tokenized_words_lemmatize']:\n",
    "        sen = []\n",
    "        for word in sentence:\n",
    "            if word.isalnum():\n",
    "                tokens.append(word)\n",
    "        \n",
    "    texts.at[index,'tokenized_words_lemmatize']  = tokens\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:00<00:00, 508.97it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in tqdm(texts_whole.iterrows(), total = len(texts_whole)):\n",
    "    tokens = []\n",
    "    for sentence in row['tokenized_words_lemmatize']:\n",
    "        sen = []\n",
    "        for word in sentence:\n",
    "            if word.isalnum():\n",
    "                tokens.append(word)\n",
    "        \n",
    "    texts_whole.at[index,'tokenized_words_lemmatize']  = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:00<00:00, 5135.15it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in tqdm(texts.iterrows(), total = len(texts)):\n",
    "    tokens = []\n",
    "    for sentence in row['tokenized_words_processed']:\n",
    "        sen = []\n",
    "        for word in sentence:\n",
    "            if word.isalnum():\n",
    "                tokens.append(word)\n",
    "        \n",
    "    texts.at[index,'tokenized_words_processed']  = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:00<00:00, 1078.21it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in tqdm(texts_whole.iterrows(), total = len(texts_whole)):\n",
    "    tokens = []\n",
    "    for sentence in row['tokenized_words_processed']:\n",
    "        sen = []\n",
    "        for word in sentence:\n",
    "            if word.isalnum():\n",
    "                tokens.append(word)\n",
    "        \n",
    "    texts_whole.at[index,'tokenized_words_processed']  = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Final dataframe contains addtionaly ( comparing to the raw one ):\n",
    "    - tokenized_sentences - content divided into sentences\n",
    "    - tokenized_words - each sentence divide into words\n",
    "    - tokenized_words_processed - content divided into words without stopwords and non-alphanumeric ones, words are stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "unique_pmids = texts['PMID'].drop_duplicates()\n",
    "train_pmids, test_pmids = train_test_split(unique_pmids, test_size=0.2)\n",
    "texts.merge(train_pmids).to_csv(os.path.join('data','data_train.csv'), index=False)\n",
    "texts.merge(test_pmids).to_csv(os.path.join('data','data_test.csv'), index=False)\n",
    "texts_whole.merge(train_pmids).to_csv(os.path.join('data','data_whole_train.csv'), index=False)\n",
    "texts_whole.merge(test_pmids).to_csv(os.path.join('data','data_whole_test.csv'), index=False)\n",
    "texts_whole.to_csv(os.path.join('data','data_whole.csv'), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envNNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69b1adaa11ceff177c5ff5a0e22271ae8b2837f309a3421a8d0197a8c1aada63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
