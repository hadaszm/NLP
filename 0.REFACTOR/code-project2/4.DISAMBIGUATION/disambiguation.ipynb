{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import math\n",
    "import datetime\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeding(word,emb):\n",
    "    \"\"\"Get word embedding \"\"\"\n",
    "    return emb.loc[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tags_list_dict(row, with_sorting = False):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : list\n",
    "        list of lists keyword, concept\n",
    "    with_sorting : bool\n",
    "        should sorting be enables\n",
    "        \n",
    "    Returns\n",
    "    ------\n",
    "    results : dict\n",
    "        initaly sorted (or random ordered) dictionary keyword : dict of concepts {concept : distance}\n",
    "    '''\n",
    "    result = {}\n",
    "    for tag in row:\n",
    "        key = tag[0].split(',')[0].upper()\n",
    "        value = tag[1].upper()\n",
    "        if key in result.keys():\n",
    "            if  value not in result[key]:\n",
    "                result[key][value]=1\n",
    "            else:\n",
    "                 result[key][value]+=1\n",
    "        else:\n",
    "            result[key]= {value:1}\n",
    "    if with_sorting:\n",
    "        for key in result.keys():\n",
    "            d = result[key]\n",
    "            d = {k:0 for k,v in dict(sorted(d.items(), key=lambda item: item[1], reverse = True)).items()}\n",
    "            result[key] = d\n",
    "    else:\n",
    "        for key in result.keys():\n",
    "            d = result[key]\n",
    "            d = {k:0 for k,v in d.items()}\n",
    "            result[key] = d\n",
    "        \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguation(current_selection,embedings, weigths, forced):\n",
    "    ''' \n",
    "    Parameters\n",
    "    ----------\n",
    "    current_selection : dict\n",
    "         dictionary keyword: list of all unique concepts\n",
    "    weigths: dict\n",
    "         the importance of given keyword\n",
    "    forced: bool\n",
    "        should the concept identical with keyword be the first\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    new_current_selction : dict\n",
    "        dictionary with new best selction of concepts\n",
    "    \n",
    "    '''\n",
    "    # we iterate over the current_selection MAX_ITER times\n",
    "    new_current_selction  = copy.deepcopy(current_selection)\n",
    "    should_stop = False\n",
    "    for i in range(7):\n",
    "\n",
    "        for keyword, concepts_list in new_current_selction.items():\n",
    "            distances = {} # for each possible concept calaculate the mean distance from other kewords (concepts of them)\n",
    "            if forced and  any([c== keyword for c in concepts_list.keys()]):\n",
    "                _ = [c != keyword for c in concepts_list.keys()]\n",
    "                distances = dict(zip(_,[1000]* len(_)))\n",
    "                distances[keyword] = 0\n",
    "            else:\n",
    "                for concept in concepts_list.keys():\n",
    "                    distances[concept] = []\n",
    "                    for k, current_best_tags in new_current_selction.items():\n",
    "                        # foreach keyword that is not a current one \n",
    "                        if k!=keyword:\n",
    "                            current_best_tag = list(current_best_tags.keys())[0] # the first out of list of concepts\n",
    "                            try:\n",
    "                                distances[concept].append(weigths[k]*math.dist(get_embeding(concept,embedings),get_embeding(current_best_tag,embedings))) # append distance from this concept\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                    distances[concept] = np.mean(distances[concept]) # mean distance \n",
    "            new_current_selction[keyword] = dict(sorted(distances.items(), key=lambda item: item[1]))  # upadate the current selection of this keyword\n",
    "    return new_current_selction\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_importance(grouped_data, tagger_data):\n",
    "    \"\"\"get keywords importance\"\"\"\n",
    "    return grouped_data.reset_index().merge(tagger_data[['PMID','topic_keywords']] ,on = 'PMID').set_index('text_to_annotate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_best_tags(data, n = 1):\n",
    "    \"\"\"how many best concepts to take\"\"\"\n",
    "    return [{k:sorted(v, key=v.get)[:n] for k,v in dd.items()} for dd in data['after_disambiguation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_disambiguation(data, tagger, embedings,column_name = 'ncbo_annotations_pairs' ,  weighting = False, sorting = False, forced = False, take_best = 1):\n",
    "    \"\"\"prepare data for disambiguation - read csv, eval, set index etc.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datas : DataFrames\n",
    "        DataFrame with keywords\n",
    "    tagger : DataFrames\n",
    "        DataFrame with keywords importance\n",
    "    embedings : DataFrames\n",
    "        DataFrame with keywords embedings\n",
    "    column_name : str\n",
    "        Name of the column with annotations\n",
    "    weigthing: bool\n",
    "        Should the weigthed voting be performed\n",
    "    sorting: bool\n",
    "        Should the initial sorting be performed\n",
    "    take_best : int\n",
    "        How many best concepts should be returned\n",
    "\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    data  : DataFrame\n",
    "    \"\"\"\n",
    "    grouped = data.groupby('text_to_annotate').nth(0)\n",
    "    # get importance for each keyword -> will be used if weighting True\n",
    "    grouped = keywords_importance(grouped, tagger )\n",
    "    grouped['possible_tags'] = grouped[column_name].apply(lambda r: create_tags_list_dict(r, sorting))\n",
    "\n",
    "    # disambiguation\n",
    "    res = []\n",
    "    for idx, row  in tqdm.tqdm(grouped.iterrows(), total = len(grouped)):\n",
    "        current_selection = row['possible_tags']\n",
    "        if not weighting:\n",
    "            weigths = dict(zip(list(row['topic_keywords'].keys()),[1] * len(row['topic_keywords'])))\n",
    "        else:\n",
    "            weigths = row['topic_keywords']\n",
    "        r = disambiguation(current_selection, embedings,weigths,forced)\n",
    "        res.append(r)\n",
    "    grouped['after_disambiguation'] = res\n",
    "    data = data.merge(grouped['after_disambiguation'].reset_index(), on = 'text_to_annotate' )\n",
    "    data['disambiguation_best_concept'] = get_n_best_tags(data, take_best)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_name, tagger_name, embedings_name):\n",
    "    \"\"\"prepare data for disambiguation - read csv, eval, set index etc.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_name : str\n",
    "        Path to get data\n",
    "    tagger_name : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    embedings_name : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    data, tagger, embedings : tuple (DataFrame, DataFrame, DataFrame) \n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(data_name)\n",
    "    data['ncbo_annotations_pairs'] = data['ncbo_annotations_pairs'].apply(eval)\n",
    "    data['ncbo_annotations_pairs']  = data['ncbo_annotations_pairs'].apply(lambda x : [[a[0].upper(),a[1]] for a in x])\n",
    "\n",
    "    tagger = pd.read_csv(tagger_name)\n",
    "    tagger['topic_keywords'] = tagger['topic_keywords'].apply(eval).apply(lambda x: {k.upper():v for k,v in dict(x).items()})\n",
    "\n",
    "\n",
    "    embedings = pd.read_csv(embedings_name)\n",
    "    embedings = embedings.set_index('words')\n",
    "    embedings.index = embedings.index.str.upper()\n",
    "    embedings = embedings[~embedings.index.duplicated(keep='first')]\n",
    "\n",
    "    return  data, tagger, embedings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_disambiguation(results_folder,data_path, tagger_path, embedings_path,timestamp,weigthing=False,sorting=False,forced = False):\n",
    "    \"\"\"Performs disambiguation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_folder : str\n",
    "        Path to save results\n",
    "    \n",
    "    data_path : str\n",
    "        Path to save the model to (folder must exist).\n",
    "\n",
    "    tagger_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    embedings_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    timestamp : str\n",
    "        Timestamp that will be added to filenames\n",
    "\n",
    "    weigthing: bool\n",
    "        Should the weigthed voting be performed\n",
    "\n",
    "    sorting: bool\n",
    "        Should the initial sorting be performed\n",
    "\n",
    "    forced: bool\n",
    "        If true: if the any concept is identical with keyword it is returned\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    result_path : str\n",
    "        Path to results\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    import pandas as pd\n",
    "    import os \n",
    "    import math\n",
    "    import numpy as np\n",
    "\n",
    "    data, tagger, embedings = prepare_data( data_path,tagger_path,embedings_path)\n",
    "    data = prepare_for_disambiguation(data,tagger,embedings,'ncbo_annotations_pairs',weigthing,sorting,forced)\n",
    "    data.to_csv(results_folder)\n",
    "\n",
    "    return results_folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 7\n",
    "data_name = '../0.RESULTS/bertopic_ncbo/bertopic_ncbo_data_2023-01-03_13-27-32.csv'\n",
    "tagger_name = '../0.RESULTS/bertopic/bertopic_lemmatize_nostopwords_data_2023-01-02_10-04-20.csv'\n",
    "embeding_name = '../0.RESULTS/embedings/ncbo_embeddings.csv'\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "weigthing = True\n",
    "sorting = True\n",
    "forced = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:07<00:00,  1.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../0.RESULTS/disambiguation/bertopic_lda_fulldata_lemmatize_nostopwords_data_2023_01_17_23_50_39_sorting_weighting_no_forcing.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_folder = f'../0.RESULTS/disambiguation/bertopic_lda_fulldata_lemmatize_nostopwords_data_{timestamp}_no_sorting_no_weighting_no_forcing.csv'\n",
    "prepare_disambiguation(results_folder,data_name, tagger_name, embeding_name,timestamp,weigthing=False,sorting=False,forced = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = f'../0.RESULTS/disambiguation/bertopic_lda_fulldata_lemmatize_nostopwords_data_{timestamp}_sorting_no_weighting_no_forcing.csv'\n",
    "prepare_disambiguation(results_folder,data_name, tagger_name, embeding_name,timestamp,weigthing=False,sorting=True,forced = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = f'../0.RESULTS/disambiguation/bertopic_lda_fulldata_lemmatize_nostopwords_data_{timestamp}_sorting_weighting_no_forcing.csv'\n",
    "prepare_disambiguation(results_folder,data_name, tagger_name, embeding_name,timestamp,weigthing=False,sorting=True,forced = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = f'../0.RESULTS/disambiguation/bertopic_lda_fulldata_lemmatize_nostopwords_data_{timestamp}_sorting_weighting_forcing.csv'\n",
    "prepare_disambiguation(results_folder,data_name, tagger_name, embeding_name,timestamp,weigthing=False,sorting=True,forced = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('envNNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69b1adaa11ceff177c5ff5a0e22271ae8b2837f309a3421a8d0197a8c1aada63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
