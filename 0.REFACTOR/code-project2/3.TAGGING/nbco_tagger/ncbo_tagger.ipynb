{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCBO functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_now_str():\n",
    "    import datetime\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def transform_strings_to_arrays(df, col_names):\n",
    "    for col in col_names:\n",
    "        df[col] = df[col].apply(eval)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_ncbo(ontologies, keywords_extractor_name, extracted_keywords_path, results_path, timestamp):\n",
    "    \"\"\"Performs NCBO tagging for keywords extracted with get_keywords_bertopic or get_keywords_lda functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ontologies : list[str]\n",
    "        List of string of ontologies ids that will be queried in tagging process.\n",
    "    \n",
    "    keywords_extractor_name : str\n",
    "        Name of the algorithm used to extract keywrods (for file/folders naming)\n",
    "\n",
    "    extracted_keywords_path : str\n",
    "        Path to the file returned by get_keywords_bertopic or get_keywords_lda functions.\n",
    "\n",
    "    results_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    timestamp : str\n",
    "        timestamp that will be added to filenames\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    save_name : str\n",
    "        Path to tagged words file.\n",
    "    \"\"\"\n",
    "\n",
    "    import urllib.request, urllib.error, urllib.parse\n",
    "    import json\n",
    "    import os\n",
    "    from pprint import pprint\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    REST_URL = \"http://data.bioontology.org\"\n",
    "    API_KEY = \"194c9635-ce67-4e70-81c5-898c3a2b30fb\"\n",
    "\n",
    "    def read_keywords_extraction_results(path):\n",
    "        data = pd.read_csv(path, index_col=0)\n",
    "        data = transform_strings_to_arrays(data, col_names = ['topic_keywords'])\n",
    "        data['text_to_annotate'] = data.topic_keywords.apply(\n",
    "            lambda row: re.sub(r\"[\\'\\[\\]]\", \"\", str([keyword[0] for keyword in row]))\n",
    "            )\n",
    "        return data\n",
    "\n",
    "    def get_json(url):\n",
    "        opener = urllib.request.build_opener()\n",
    "        opener.addheaders = [('Authorization', 'apikey token=' + API_KEY)]\n",
    "        return json.loads(opener.open(url).read())\n",
    "\n",
    "    def create_annotation_pairs(sample_row, column_name):\n",
    "        found_concepts = sample_row[column_name]\n",
    "        res_ann_pairs= []\n",
    "        for _, concept in enumerate(found_concepts):\n",
    "            max_trials = 5\n",
    "            trials_no = 0\n",
    "            while trials_no < max_trials:\n",
    "                try:\n",
    "                    concept_class = get_json(concept[\"annotatedClass\"][\"links\"][\"self\"])\n",
    "                    concept_class_ancestors = get_json(concept[\"annotatedClass\"]['links']['ancestors'])\n",
    "                    break\n",
    "                except:\n",
    "                    trials_no+=1\n",
    "                    continue\n",
    "            if trials_no==max_trials:\n",
    "                raise Exception(\"number of unsuccessfull connection attempts is max_trials\")\n",
    "            annotations = concept['annotations']\n",
    "            # annotations for this class\n",
    "            for annot in annotations:\n",
    "                res_ann_pairs.append([annot['text'], concept_class[\"prefLabel\"], 'DIRECT', concept[\"annotatedClass\"][\"links\"][\"self\"]])\n",
    "            # annotations for ancestors\n",
    "            for annot in annotations:\n",
    "                for ancestor in concept_class_ancestors:\n",
    "                    res_ann_pairs.append([annot['text'], ancestor[\"prefLabel\"], 'ANCESTOR', concept[\"annotatedClass\"]['links']['ancestors']])\n",
    "        unique_ann_pairs = [list(x) for x in set(tuple(x) for x in res_ann_pairs)]\n",
    "        return unique_ann_pairs\n",
    "\n",
    "        \n",
    "    ##########################################################################################################################\n",
    "\n",
    "    # read data\n",
    "    data = read_keywords_extraction_results(extracted_keywords_path)\n",
    "\n",
    "    # annotate data\n",
    "    data['ncbo_annotations'] \\\n",
    "        = data.text_to_annotate.apply(lambda text:  \\\n",
    "            get_json(REST_URL + f\"/annotator?ontologies={','.join(ontologies)}&text=\" + urllib.parse.quote(text)))\n",
    "\n",
    "    data = data.reset_index()[['PMID', 'text_to_annotate', 'ncbo_annotations']]\n",
    "\n",
    "    data_to_annotate = data[['text_to_annotate', 'ncbo_annotations']]\n",
    "    data_to_annotate = data_to_annotate.loc[data_to_annotate.astype(str).drop_duplicates().index]\n",
    "    data_to_annotate['ncbo_annotation_pairs'] = data_to_annotate.apply(create_annotation_pairs, column_name='ncbo_annotations', axis = 1)\n",
    "\n",
    "    # create annotation pairs\n",
    "    data_to_annotate[['text_to_annotate', 'ncbo_annotation_pairs']].to_dict()\n",
    "    text_to_annot_ncbo_pairs = dict(zip(data_to_annotate.text_to_annotate, data_to_annotate.ncbo_annotation_pairs))\n",
    "    data['ncbo_annotations_pairs'] = data['text_to_annotate'].apply(lambda text: text_to_annot_ncbo_pairs[text])\n",
    "\n",
    "    # save data\n",
    "    res_folder = f'{results_path}/{keywords_extractor_name}_ncbo'\n",
    "    if not os.path.exists(res_folder):\n",
    "        os.makedirs(res_folder)\n",
    "    save_name = f'{res_folder}/{keywords_extractor_name}_ncbo_{timestamp}.csv'\n",
    "    data.to_csv(save_name, index=False)\n",
    "\n",
    "    return save_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRAFT tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bertopic based keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../0.RESULTS//bertopic_ncbo/bertopic_ncbo_2023-01-17_23-26-28.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRAFT_ONTOLOGIES = ['CHEBI', 'CL', 'GO', 'MONDO', 'MOP', 'NCBITAXON', 'PR', 'SO', 'UBERON']\n",
    "keywords_extractor_name = 'bertopic'\n",
    "extracted_keywords_path = f'../../0.RESULTS/bertopic/bertopic_lemmatize_nostopwords_data_2023-01-17_23-06-45.csv'\n",
    "results_path = '../../0.RESULTS/'\n",
    "timestamp = get_now_str()\n",
    "\n",
    "tag_ncbo(CRAFT_ONTOLOGIES, keywords_extractor_name, extracted_keywords_path, results_path, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lda based keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../0.RESULTS//lda_ncbo/lda_ncbo_2023-01-17_23-48-04.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRAFT_ONTOLOGIES = ['CHEBI', 'CL', 'GO', 'MONDO', 'MOP', 'NCBITAXON', 'PR', 'SO', 'UBERON']\n",
    "keywords_extractor_name = 'lda'\n",
    "extracted_keywords_path = f'../../0.RESULTS/lda/lda_results_2023-01-17_22-31-53.csv'\n",
    "results_path = '../../0.RESULTS/'\n",
    "timestamp = get_now_str()\n",
    "\n",
    "tag_ncbo(CRAFT_ONTOLOGIES, keywords_extractor_name, extracted_keywords_path, results_path, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88ba28277a1630fc5f744f385ba08b8a5477e64ab64787e3b7b12637e39579c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
