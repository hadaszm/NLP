{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6186f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e95bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c8391290",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_glob_regex = '../../0.RESULTS/embeddings/*_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ec41667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(TP, FP): \n",
    "    return TP/(TP+FP)\n",
    "\n",
    "def recall(TP, FN): \n",
    "    return TP/(TP+FN)\n",
    "\n",
    "def F1(precision, recall):\n",
    "    return 2/(1/precision + 1/recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6a3c0556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_TP_FN_FP(keywords, ground_truth):\n",
    "    keywords = set(keywords)\n",
    "    ground_truth = set(ground_truth)\n",
    "    TP = len(keywords.intersection(ground_truth) )\n",
    "    FN = len(keywords.difference(ground_truth))\n",
    "    FP = len(ground_truth.difference(keywords))\n",
    "    return np.array([TP, FN, FP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b3d33618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_list(pmid, pairs):\n",
    "    result = []\n",
    "    for pair in pairs:\n",
    "        result.append((pmid, pair[0], pair[1]))\n",
    "    return result\n",
    "\n",
    "def explode_df(df): \n",
    "    result = []\n",
    "    for i, row in df.iterrows(): \n",
    "        result += explode_list(row['PMID'], eval(row['ncbo_annotations_pairs']))\n",
    "    return pd.DataFrame(result, columns=['PMID', 'keyword', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e94ff6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_id(id_): \n",
    "    if ':' in id_:\n",
    "        id_ = id_.split(':')\n",
    "    else: \n",
    "        id_ = id_.split('_')\n",
    "    try:\n",
    "        id_[1] = int(id_[1])\n",
    "    except (ValueError, IndexError) as e: \n",
    "        pass\n",
    "    return tuple(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "62a6a8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MentionTextSegment</th>\n",
       "      <th>id</th>\n",
       "      <th>PMID</th>\n",
       "      <th>ontology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>biological pigment</td>\n",
       "      <td>(chebi, 26130)</td>\n",
       "      <td>11532192</td>\n",
       "      <td>chebi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cation</td>\n",
       "      <td>(chebi, 36916)</td>\n",
       "      <td>11897010</td>\n",
       "      <td>chebi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ion</td>\n",
       "      <td>(chebi, 24870)</td>\n",
       "      <td>11897010</td>\n",
       "      <td>chebi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chromogenic compound</td>\n",
       "      <td>(chebi, 75050)</td>\n",
       "      <td>12079497</td>\n",
       "      <td>chebi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>electron</td>\n",
       "      <td>(chebi, 10545)</td>\n",
       "      <td>12546709</td>\n",
       "      <td>chebi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MentionTextSegment              id      PMID ontology\n",
       "0    biological pigment  (chebi, 26130)  11532192    chebi\n",
       "1                cation  (chebi, 36916)  11897010    chebi\n",
       "2                   ion  (chebi, 24870)  11897010    chebi\n",
       "3  chromogenic compound  (chebi, 75050)  12079497    chebi\n",
       "4              electron  (chebi, 10545)  12546709    chebi"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = []\n",
    "for file in glob.glob('../0.RESULTS/preprocessing/*/annotations.csv'): \n",
    "    onto = os.path.basename(os.path.dirname(file))\n",
    "    if 'GO_' in onto:\n",
    "        onto = 'GO'\n",
    "    tmp = pd.read_csv(file)\n",
    "    tmp['ontology'] = onto.lower()\n",
    "    annotations.append(tmp)\n",
    "annotations = pd.concat(annotations).drop(['Unnamed: 0','StartIndex','EndIndex'],axis=1)\\\n",
    "    .rename({'EntityID': 'id'}, axis=1)\n",
    "annotations['id'] = annotations['id'].apply(lambda x: x.lower()).apply(process_id)\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "87c70c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chebi', 'cl', 'go', 'mop', 'ncbitaxon', 'pr', 'so', 'uberon'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(annotations['id'].apply(lambda x: x[0]).drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "05e3d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>keyword</th>\n",
       "      <th>concept</th>\n",
       "      <th>id</th>\n",
       "      <th>ontology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PMID, keyword, concept, id, ontology]\n",
       "Index: []"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags = pd.concat([pd.read_csv(file) for file in glob.glob('../CRAFT/results/emb_tagger/*')])\n",
    "all_tags[all_tags['id'].isin([\n",
    "'1058',\n",
    "'8620',\n",
    "'BFO_0000023',\n",
    "'BFO_0000034',\n",
    "'CARO_0000013',\n",
    "'ENVO_01000739',\n",
    "'FOODON_00003004',\n",
    "'MGI:1890498',\n",
    "'MONDO_0021140',\n",
    "'MONDO_0021152',\n",
    "'NCBITaxon_family', \n",
    "'UPHENO_0001001',\n",
    "'WBGene00003937',\n",
    "'WBGene00004950',\n",
    "'WBGene00006560',\n",
    "'WBGene00011828',\n",
    "'main.html?id=1591873',\n",
    "'main.html?id=2666',\n",
    "'main.html?id=620991'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "875d5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_groupped = annotations.groupby('PMID').apply(lambda x: \n",
    "                                 pd.Series({'true_ids': set(x['id'])}, index=['true_ids'])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "648fd034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bertopic_tagged.csv</td>\n",
       "      <td>5.711264</td>\n",
       "      <td>11.933702</td>\n",
       "      <td>7.725322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lda_tagged.csv</td>\n",
       "      <td>4.019038</td>\n",
       "      <td>8.315098</td>\n",
       "      <td>5.418895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file  precision     recall        F1\n",
       "0  bertopic_tagged.csv   5.711264  11.933702  7.725322\n",
       "1       lda_tagged.csv   4.019038   8.315098  5.418895"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = []\n",
    "for file in glob.glob('../0.RESULTS/emb_tagger/*'):\n",
    "    tagged = pd.read_csv(file)\n",
    "    tagged['id'] = tagged['id'].apply(lambda x: x.lower()).apply(process_id)\n",
    "    tagged_groupped = tagged.groupby('PMID').apply(lambda x: \n",
    "                                 pd.Series({'pred_ids': set(x['id'])}, index=['pred_ids'])).reset_index()\n",
    "    merged = tagged_groupped.merge(annotations_groupped,how='outer')\n",
    "    \n",
    "    \n",
    "    TP_FN_FP_ids = np.zeros((3,))\n",
    "    for i, row in merged.iterrows(): \n",
    "        TP_FN_FP_ids += return_TP_FN_FP(row['pred_ids'], row['true_ids'])\n",
    "    \n",
    "    precision_ = precision(TP_FN_FP_ids[0],TP_FN_FP_ids[2])\n",
    "    recall_ = recall(TP_FN_FP_ids[0], TP_FN_FP_ids[1])\n",
    "    stats.append((os.path.basename(file), precision_*100, recall_*100, 100*F1(precision_, recall_)))\n",
    "pd.DataFrame(stats, columns=['file', 'precision', 'recall', 'F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b86aac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontologies_mappings = pd.concat([pd.read_csv(file) for file in glob.glob('../CRAFT/data/ontologies_mappings/*')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b5670f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ncbo_concepts_used = pd.concat([pd.read_csv(file) for file in glob.glob('../CRAFT/results/disambiguation/*')])\n",
    "text_to_id = dict()\n",
    "for elem in sum(all_ncbo_concepts_used['ncbo_annotations'].apply(eval), []):\n",
    "    id_ = elem['annotatedClass']['@id'].split('/')[-1]\n",
    "    for annotation in elem['annotations']:\n",
    "        text_to_id[annotation['text'].lower()] = id_\n",
    "all_ncbo_concepts_used = {elem['annotatedClass']['@id'].split('/')[-1] for elem in sum(all_ncbo_concepts_used['ncbo_annotations'].apply(eval), [])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e999dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_concepts = ontologies_mappings[ontologies_mappings['id'].isin(all_ncbo_concepts_used)].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1704ded8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T016', 'T025', 'T192'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ncbo_concepts_used.difference(filtered_concepts['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2e5d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontologies_mappings['label'] = ontologies_mappings['label'].apply(lambda x: x.lower())\n",
    "label_to_ids = ontologies_mappings.groupby('label').apply(lambda x: list(set(x['id']))).reset_index().rename({0: 'ids'}, axis=1)\n",
    "label_to_ids = dict(zip(label_to_ids['label'], label_to_ids['ids']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b2fb9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_id(id_): \n",
    "    if ':' in id_:\n",
    "        id_ = id_.split(':')\n",
    "    else: \n",
    "        id_ = id_.split('_')\n",
    "    try:\n",
    "        id_[1] = int(id_[1])\n",
    "    except (ValueError, IndexError) as e: \n",
    "        pass\n",
    "    return tuple(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "53599fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CELL': ['BIOLOGICAL ENTITY'],\n",
       " 'MOUSE': ['MATERIAL ENTITY'],\n",
       " 'EMBRYO': ['MATERIAL ENTITY'],\n",
       " 'EMBRYONIC': ['MATERIAL ENTITY'],\n",
       " 'DNA': ['MATERIAL ENTITY'],\n",
       " 'GENE': ['BIOLOGICAL_REGION']}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(file)['disambiguation_best_concept'].apply(eval).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "972d0adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMID     710\n",
      "label    710\n",
      "id       705\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_8564\\269410256.py:8: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return 2/(1/precision + 1/recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMID     710\n",
      "label    710\n",
      "id       705\n",
      "dtype: int64\n",
      "PMID     710\n",
      "label    710\n",
      "id       705\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_8564\\269410256.py:8: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return 2/(1/precision + 1/recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMID     710\n",
      "label    710\n",
      "id       705\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_8564\\269410256.py:8: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return 2/(1/precision + 1/recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMID     710\n",
      "label    710\n",
      "id       705\n",
      "dtype: int64\n",
      "PMID     589\n",
      "label    589\n",
      "id       559\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_8564\\269410256.py:8: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return 2/(1/precision + 1/recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMID     589\n",
      "label    589\n",
      "id       559\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_8564\\269410256.py:8: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return 2/(1/precision + 1/recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMID     589\n",
      "label    589\n",
      "id       559\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_8564\\269410256.py:8: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return 2/(1/precision + 1/recall)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_03_18_02_32_train_sorted.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_03_18_02_32_weigthed_and_sorted.csv</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.001689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_no_sorted_no_weighted.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_sorted_no_weighted.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_weighted_and_sorted.csv</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.001689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_no_sorted_no_weighted.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_sorted_no_weighted.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_weighted_and_sorted.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              file  \\\n",
       "0           bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_03_18_02_32_train_sorted.csv   \n",
       "1    bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_03_18_02_32_weigthed_and_sorted.csv   \n",
       "2  bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_no_sorted_no_weighted.csv   \n",
       "3     bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_sorted_no_weighted.csv   \n",
       "4    bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_weighted_and_sorted.csv   \n",
       "5       lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_no_sorted_no_weighted.csv   \n",
       "6          lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_sorted_no_weighted.csv   \n",
       "7         lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_weighted_and_sorted.csv   \n",
       "\n",
       "   precision    recall        F1  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "1   0.001058  0.004193  0.001689  \n",
       "2   0.000000  0.000000  0.000000  \n",
       "3   0.000000  0.000000  0.000000  \n",
       "4   0.001058  0.004193  0.001689  \n",
       "5   0.000000  0.000000  0.000000  \n",
       "6   0.000000  0.000000  0.000000  \n",
       "7   0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = []\n",
    "for file in glob.glob('../0.RESULTS/disambiguation/*'):\n",
    "    dis = pd.read_csv(file) \n",
    "    dis['disambiguation_best_concept'] = dis['disambiguation_best_concept'].apply(eval).apply(lambda x: [v[0].lower() for v in x.values()])\n",
    "    dis = dis[['PMID', 'disambiguation_best_concept']].explode('disambiguation_best_concept')\\\n",
    "        .rename({'disambiguation_best_concept': 'label'}, axis=1)\n",
    "    \n",
    "    dis['id'] = dis['label'].map(label_to_ids)\n",
    "    print(dis.count())\n",
    "    dis['id'] = dis['id'].apply(lambda x: x if x == x else [])\n",
    "\n",
    "    dis['id'] = dis['id'].apply(lambda x: [process_id(elem.lower()) for elem in x])\n",
    "    dis = dis.groupby('PMID').apply(lambda x: \n",
    "                     pd.Series({'pred_ids': set(sum(x['id'], []))}, index=['pred_ids'])).reset_index()\n",
    "    merged = dis.merge(annotations_groupped,on=['PMID'], how='outer')\n",
    "    \n",
    "    TP_FN_FP_ids = np.zeros((3,))\n",
    "    for i, row in merged.iterrows(): \n",
    "        TP_FN_FP_ids += return_TP_FN_FP(row['pred_ids'], row['true_ids'])\n",
    "    \n",
    "    precision_ = precision(TP_FN_FP_ids[0],TP_FN_FP_ids[2])\n",
    "    recall_ = recall(TP_FN_FP_ids[0], TP_FN_FP_ids[1])\n",
    "    stats.append((os.path.basename(file), precision_, recall_, F1(precision_, recall_)))\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(stats, columns=['file', 'precision', 'recall', 'F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437eb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1fc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45157ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c057315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c69df14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_03_18_02_32_train_sorted.csv</td>\n",
       "      <td>0.052882</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.076894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_03_18_02_32_weigthed_and_sorted.csv</td>\n",
       "      <td>0.052882</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.076894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_no_sorted_no_weighted.csv</td>\n",
       "      <td>0.052882</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.076894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_sorted_no_weighted.csv</td>\n",
       "      <td>0.052882</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.076894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_weighted_and_sorted.csv</td>\n",
       "      <td>0.052882</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.076894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_no_sorted_no_weighted.csv</td>\n",
       "      <td>0.042834</td>\n",
       "      <td>0.137521</td>\n",
       "      <td>0.065323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_sorted_no_weighted.csv</td>\n",
       "      <td>0.042834</td>\n",
       "      <td>0.137521</td>\n",
       "      <td>0.065323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_weighted_and_sorted.csv</td>\n",
       "      <td>0.042834</td>\n",
       "      <td>0.137521</td>\n",
       "      <td>0.065323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              file  \\\n",
       "0           bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_03_18_02_32_train_sorted.csv   \n",
       "1    bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_03_18_02_32_weigthed_and_sorted.csv   \n",
       "2  bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_no_sorted_no_weighted.csv   \n",
       "3     bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_sorted_no_weighted.csv   \n",
       "4    bertopic_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_51_30_weighted_and_sorted.csv   \n",
       "5       lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_no_sorted_no_weighted.csv   \n",
       "6          lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_sorted_no_weighted.csv   \n",
       "7         lda_ncbo_fulldata_lemmatize_nostopwords_data_2023_01_04_17_37_13_weighted_and_sorted.csv   \n",
       "\n",
       "   precision    recall        F1  \n",
       "0   0.052882  0.140845  0.076894  \n",
       "1   0.052882  0.140845  0.076894  \n",
       "2   0.052882  0.140845  0.076894  \n",
       "3   0.052882  0.140845  0.076894  \n",
       "4   0.052882  0.140845  0.076894  \n",
       "5   0.042834  0.137521  0.065323  \n",
       "6   0.042834  0.137521  0.065323  \n",
       "7   0.042834  0.137521  0.065323  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = []\n",
    "for file in glob.glob('../0.RESULTS/disambiguation/*'):\n",
    "    dis = pd.read_csv(file) \n",
    "    dis['disambiguation_best_concept'] = dis['disambiguation_best_concept'].apply(eval).apply(lambda x: [k.lower() for k in x.keys()])\n",
    "    dis = dis[['PMID', 'disambiguation_best_concept']].explode('disambiguation_best_concept')\\\n",
    "        .rename({'disambiguation_best_concept': 'label'}, axis=1)\n",
    "    dis['id'] = dis['label'].map(text_to_id).apply(lambda x: process_id(x.lower()))\n",
    "    dis = dis.groupby('PMID').apply(lambda x: \n",
    "                     pd.Series({'pred_ids': set(x['id'])}, index=['pred_ids'])).reset_index()\n",
    "    merged = dis.merge(annotations_groupped,on=['PMID'], how='outer')\n",
    "    \n",
    "    TP_FN_FP_ids = np.zeros((3,))\n",
    "    for i, row in merged.iterrows(): \n",
    "        TP_FN_FP_ids += return_TP_FN_FP(row['pred_ids'], row['true_ids'])\n",
    "    \n",
    "    precision_ = precision(TP_FN_FP_ids[0],TP_FN_FP_ids[2])\n",
    "    recall_ = recall(TP_FN_FP_ids[0], TP_FN_FP_ids[1])\n",
    "    stats.append((os.path.basename(file), precision_, recall_, F1(precision_, recall_)))\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(stats, columns=['file', 'precision', 'recall', 'F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3ee7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
