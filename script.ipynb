{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_now_str():\n",
    "    import datetime\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def transform_strings_to_arrays(df, col_names):\n",
    "    for col in col_names:\n",
    "        df[col] = df[col].apply(eval)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_bertopic(data_path, models_path, results_path, timestamp):\n",
    "    \"\"\"Performs bertopic keywords extraction for data after lemmatization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to preprocessed dataset. Dataset must contain a column with name 'tokenized_words_lemmatize'.\n",
    "    \n",
    "    models_path : str\n",
    "        Path to save the model to (folder must exist).\n",
    "\n",
    "    results_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    timestamp : str\n",
    "        timestamp that will be added to filenames\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    (result_path, model_save_name) : tuple[str]\n",
    "        Frist element is the path to created file with extracted keywrods, second - path to created model.\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    from bertopic import BERTopic\n",
    "    import os \n",
    "    import numpy as np\n",
    "\n",
    "    # basic BertTopic keyword extraction\n",
    "    def train_transform_save(train_data, model_save_name, min_topic_size=10):\n",
    "        \n",
    "        # train transform\n",
    "        topic_model = BERTopic(min_topic_size=min_topic_size)\n",
    "        topics, probs = topic_model.fit_transform(train_data.values)\n",
    "\n",
    "        # save model\n",
    "        topic_model.save(model_save_name)\n",
    "\n",
    "        return topic_model, topics, probs\n",
    "\n",
    "\n",
    "    def load_transform_save(data, model_save_name, results_path):\n",
    "\n",
    "        # load model\n",
    "        loaded_model = BERTopic.load(model_save_name)\n",
    "\n",
    "        # transform for data \n",
    "        samples_topics, samples_probs = loaded_model.transform(data.values)\n",
    "        res_df = pd.DataFrame({\n",
    "            'PMID': np.unique(data.index),\n",
    "            'topic_number': samples_topics,\n",
    "            'topic_probs': samples_probs,\n",
    "            \"topic_keywords\": [loaded_model.get_topic(topic_number) for topic_number in samples_topics]\n",
    "        })\n",
    "        res_df.to_csv(results_path, index=False)\n",
    "        return loaded_model, res_df\n",
    "\n",
    "    ##############################################################################################################################\n",
    "\n",
    "    full_data = transform_strings_to_arrays(pd.read_csv(data_path), col_names=['tokenized_words_lemmatize'])\n",
    "\n",
    "    data = full_data.groupby(by = ['PMID'])['tokenized_words_lemmatize'].agg(lambda x: ' '.join(x.values[0] + x.values[1]))\n",
    "\n",
    "    model_name = f'bertopic_keywords_{timestamp}'\n",
    "    model_save_name = os.path.join(models_path, model_name)\n",
    "    result_path = os.path.join(results_path, 'bertopic', f'{model_name}.csv')\n",
    "\n",
    "    topic_model, topics, probs = train_transform_save(data, model_save_name, min_topic_size=3)\n",
    "    _, res_df = load_transform_save(data, model_save_name, result_path)\n",
    "\n",
    "    return result_path, model_save_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_ncbo(ontologies, keywords_extractor_name, extracted_keywords_path, results_path, timestamp):\n",
    "    \"\"\"Performs NCBO tagging for keywords extracted with get_keywords_bertopic or get_keywords_lda functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ontologies : list[str]\n",
    "        List of string of ontologies ids that will be queried in tagging process.\n",
    "    \n",
    "    keywords_extractor_name : str\n",
    "        Name of the algorithm used to extract keywrods (for file/folders naming)\n",
    "\n",
    "    extracted_keywords_path : str\n",
    "        Path to the file returned by get_keywords_bertopic or get_keywords_lda functions.\n",
    "\n",
    "    results_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    timestamp : str\n",
    "        timestamp that will be added to filenames\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    save_name : str\n",
    "        Path to tagged words file.\n",
    "    \"\"\"\n",
    "\n",
    "    import urllib.request, urllib.error, urllib.parse\n",
    "    import json\n",
    "    import os\n",
    "    from pprint import pprint\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    REST_URL = \"http://data.bioontology.org\"\n",
    "    API_KEY = \"194c9635-ce67-4e70-81c5-898c3a2b30fb\"\n",
    "\n",
    "    def read_keywords_extraction_results(path):\n",
    "        data = pd.read_csv(path, index_col=0)\n",
    "        data = transform_strings_to_arrays(data, col_names = ['topic_keywords'])\n",
    "        data['text_to_annotate'] = data.topic_keywords.apply(\n",
    "            lambda row: re.sub(r\"[\\'\\[\\]]\", \"\", str([keyword[0] for keyword in row]))\n",
    "            )\n",
    "        return data\n",
    "\n",
    "    def get_json(url):\n",
    "        opener = urllib.request.build_opener()\n",
    "        opener.addheaders = [('Authorization', 'apikey token=' + API_KEY)]\n",
    "        return json.loads(opener.open(url).read())\n",
    "\n",
    "    def create_annotation_pairs(sample_row, column_name):\n",
    "        found_concepts = sample_row[column_name]\n",
    "        res_ann_pairs= []\n",
    "        for _, concept in enumerate(found_concepts):\n",
    "            max_trials = 5\n",
    "            trials_no = 0\n",
    "            while trials_no < max_trials:\n",
    "                try:\n",
    "                    concept_class = get_json(concept[\"annotatedClass\"][\"links\"][\"self\"])\n",
    "                    concept_class_ancestors = get_json(concept[\"annotatedClass\"]['links']['ancestors'])\n",
    "                    break\n",
    "                except:\n",
    "                    trials_no+=1\n",
    "                    continue\n",
    "            if trials_no==max_trials:\n",
    "                raise Exception(\"number of unsuccessfull connection attempts is max_trials\")\n",
    "            annotations = concept['annotations']\n",
    "            # annotations for this class\n",
    "            for annot in annotations:\n",
    "                res_ann_pairs.append([annot['text'], concept_class[\"prefLabel\"], 'DIRECT', concept[\"annotatedClass\"][\"links\"][\"self\"]])\n",
    "            # annotations for ancestors\n",
    "            for annot in annotations:\n",
    "                for ancestor in concept_class_ancestors:\n",
    "                    res_ann_pairs.append([annot['text'], ancestor[\"prefLabel\"], 'ANCESTOR', concept[\"annotatedClass\"]['links']['ancestors']])\n",
    "        unique_ann_pairs = [list(x) for x in set(tuple(x) for x in res_ann_pairs)]\n",
    "        return unique_ann_pairs\n",
    "\n",
    "        \n",
    "    ##########################################################################################################################\n",
    "\n",
    "    # read data\n",
    "    data = read_keywords_extraction_results(extracted_keywords_path)\n",
    "\n",
    "    # annotate data\n",
    "    data['ncbo_annotations'] \\\n",
    "        = data.text_to_annotate.apply(lambda text:  \\\n",
    "            get_json(REST_URL + f\"/annotator?ontologies={','.join(ontologies)}&text=\" + urllib.parse.quote(text)))\n",
    "\n",
    "    data = data.reset_index()[['PMID', 'text_to_annotate', 'ncbo_annotations']]\n",
    "\n",
    "    data_to_annotate = data[['text_to_annotate', 'ncbo_annotations']]\n",
    "    data_to_annotate = data_to_annotate.loc[data_to_annotate.astype(str).drop_duplicates().index]\n",
    "    data_to_annotate['ncbo_annotation_pairs'] = data_to_annotate.apply(create_annotation_pairs, column_name='ncbo_annotations', axis = 1)\n",
    "\n",
    "    # create annotation pairs\n",
    "    data_to_annotate[['text_to_annotate', 'ncbo_annotation_pairs']].to_dict()\n",
    "    text_to_annot_ncbo_pairs = dict(zip(data_to_annotate.text_to_annotate, data_to_annotate.ncbo_annotation_pairs))\n",
    "    data['ncbo_annotations_pairs'] = data['text_to_annotate'].apply(lambda text: text_to_annot_ncbo_pairs[text])\n",
    "\n",
    "    # save data\n",
    "    res_folder = f'{results_path}/{keywords_extractor_name}_ncbo'\n",
    "    if not os.path.exists(res_folder):\n",
    "        os.makedirs(res_folder)\n",
    "    save_name = f'{res_folder}/{keywords_extractor_name}_ncbo_{timestamp}.csv'\n",
    "    data.to_csv(save_name, index=False)\n",
    "\n",
    "    return save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_folder,results_folder,option):\n",
    "    \"\"\"Performs bertopic keywords extraction for data after lemmatization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_folder : str\n",
    "        Path to folder with files with data (filenames are hardcoded)\n",
    "\n",
    "    results_folder : str\n",
    "        Path to folder where results will be saved (folder must exists)\n",
    "\n",
    "    option : str\n",
    "        MedM if MedMentions dataset, CRAFT if craft\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    result_path : str\n",
    "        Path to results\n",
    "    \"\"\"\n",
    "    import gzip\n",
    "    import pandas as pd\n",
    "    import tqdm\n",
    "    import re\n",
    "    import os\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import xml.etree.ElementTree as ET\n",
    "    from tqdm import tqdm\n",
    "  \n",
    "\n",
    "    def parse(file_path:str, data_columns:list,annotations_columns:list) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        data = pd.DataFrame(columns = data_columns)\n",
    "        annotations = pd.DataFrame(columns = annotations_columns)\n",
    "        errors = pd.DataFrame()\n",
    "        # hardcoded -  differentiate if the line contains content or annotation\n",
    "        # if HEADED it is content\n",
    "        HEADER = re.compile(r\"(?P<PMID>[0-9]*)\\|(?P<Type>[t|a])\\|(?P<Content>.*)\")\n",
    "        with gzip.open(file_path, 'rb') as f:\n",
    "            i = 0\n",
    "            for line in tqdm(f.readlines()):\n",
    "                i+=1\n",
    "                l = line.decode(\"utf-8\")\n",
    "                if l == '\\n':\n",
    "                    continue\n",
    "                h = HEADER.match(l)\n",
    "                if h:\n",
    "                    data = pd.concat([data,pd.DataFrame([{k:h.group(k) for k in data_columns}])], ignore_index=True)\n",
    "                else:\n",
    "                    _ = l.split('\\t')\n",
    "                    if len(_) == len(annotations_columns):\n",
    "                        annotations = pd.concat([annotations,pd.DataFrame([dict(zip(annotations_columns,_))])], ignore_index=True)\n",
    "                    else:\n",
    "                        errors = pd.concat([errors,pd.DataFrame([l])],ignore_index=True)\n",
    "        return data, annotations,errors\n",
    "\n",
    "\n",
    "    def process_lemma(lemmatizer,sentence: list) -> list:\n",
    "        \"\"\"\n",
    "        takes list of tokens and returns steamed tokens without stopwords\n",
    "        If word contains non-letters it appends it to the final list\n",
    "        \"\"\"\n",
    "        processed = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                word_lower = word.lower() \n",
    "                if word_lower not in stopwords.words():\n",
    "                    processed.append(lemmatizer.lemmatize(word))\n",
    "            except TypeError: # when word contains non-letters\n",
    "                processed.append(word)\n",
    "        return processed\n",
    "\n",
    "\n",
    "    def get_folder_for_ontology(ontology):\n",
    "        folder = os.path.join('concept-annotation',ontology,ontology,'knowtator')\n",
    "        return folder\n",
    "\n",
    "\n",
    "    def get_data_from_file(root):\n",
    "        annotations = pd.DataFrame(columns=['StartIndex', 'EndIndex','MentionTextSegment','EntityID'])\n",
    "        for child in root:\n",
    "        # annotation\n",
    "            if child.tag=='annotation':\n",
    "                tmp = {}\n",
    "                id_name = None\n",
    "                for c in child:\n",
    "                    if c.tag == 'mention':\n",
    "                        id_name = c.attrib['id']\n",
    "                    elif c.tag == 'span':\n",
    "                        tmp['StartIndex'] = c.attrib['start']\n",
    "                        tmp['EndIndex'] = c.attrib['end']\n",
    "                annotations.loc[id_name,['StartIndex','EndIndex']] = tmp\n",
    "\n",
    "        # classmention\n",
    "            else:\n",
    "                id_name =child.attrib['id']\n",
    "                tmp = {}\n",
    "                for c in child:\n",
    "                    if c.tag == 'mentionClass' and 'id' in c.attrib.keys():\n",
    "                        tmp['MentionTextSegment'] = c.text\n",
    "                        tmp['EntityID'] = c.attrib['id']\n",
    "                        annotations.loc[id_name,['MentionTextSegment','EntityID']] = tmp\n",
    "        return annotations\n",
    "\n",
    "\n",
    "\n",
    "    def get_data(texts,ontology,file_name):\n",
    "        folder = get_folder_for_ontology(ontology)\n",
    "        folder = os.path.join(data_folder,folder)\n",
    "        data = pd.DataFrame()\n",
    "        for file in os.listdir(folder):\n",
    "            tree = ET.parse(os.path.join(folder,file))\n",
    "            root = tree.getroot()\n",
    "            annotations = get_data_from_file(root)\n",
    "            annotations['PMID'] = file[:8]\n",
    "            data = pd.concat([data,annotations])\n",
    "        file_path = os.path.join('data',ontology,file_name)\n",
    "        isExist = os.path.exists(os.path.join('data',ontology))\n",
    "        if not isExist:\n",
    "            os.makedirs(os.path.join('data',ontology))\n",
    "        data.to_csv(file_path)\n",
    "        return data\n",
    "\n",
    "\n",
    "    \n",
    "    data_columns = ['PMID', 'Type','Content']\n",
    "    annotations_columns = ['PMID', 'StartIndex','EndIndex','MentionTextSegment','SemanticTypeID','EntityID']\n",
    "\n",
    "    if option == 'MedM':\n",
    "\n",
    "        data_21, annotations_21,errors_21 = parse(os.path.join(data_folder,'corpus_pubtator.txt.gz'), data_columns, annotations_columns)\n",
    "        semantic_mapping = pd.read_csv(os.path.join(data_folder, 'semantic_type_mapping.txt'), sep = '|', header=None)[[1,2]]\n",
    "        semanitc_mapper = dict(zip(semantic_mapping[1],semantic_mapping[2] ))\n",
    "        annotations_21['EntityID'] = annotations_21['EntityID'].apply(lambda x : x.replace('\\n',''))\n",
    "        annotations_21['SemanticMeaning'] = annotations_21['SemanticTypeID'].apply(lambda x : semanitc_mapper[x])\n",
    "        data_21 = data_21.reset_index()\n",
    "        annotations_21 = annotations_21.reset_index()\n",
    "        annotations_21.to_csv(os.path.join(results_folder,'annotations.csv'), index=False)\n",
    "\n",
    "\n",
    "    if option == 'CRAFT':\n",
    "        articles_folder = os.path.join('data','articles','txt')\n",
    "        data_21 = pd.DataFrame(columns = ['PMID','Type','Content'])\n",
    "        for i,file in enumerate(os.listdir(articles_folder)):\n",
    "            if file[-3:] == 'txt':\n",
    "                name = file[:-4]\n",
    "                with open(os.path.join(articles_folder,file),'r',encoding='utf-8') as f:\n",
    "                    lines = list(f.readlines())\n",
    "                    data_21.loc[len(data_21)] = {'PMID':name, 'Type':'t','Content':lines[0]}\n",
    "                    data_21.loc[len(data_21)] = {'PMID':name, 'Type':'a','Content':''.join(lines[1:])}\n",
    "\n",
    "        for ontology in os.listdir(os.path.join('data','concept-annotation')):\n",
    "            get_data(data_21,ontology,'annotations.csv')\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # sentences\n",
    "    data_21['tokenized_sentences'] = data_21['Content'].apply(lambda text : nltk.sent_tokenize(text))\n",
    "    data_21['tokenized_words'] = None\n",
    "    data_21['tokenized_words_lemmatize'] = None\n",
    "    for index, row in tqdm(data_21.iterrows(), total = len(data_21)):\n",
    "        tokens = []\n",
    "        tokens_lemma = []\n",
    "        for sentence in row['tokenized_sentences']:\n",
    "            tok_sen = nltk.word_tokenize(sentence)\n",
    "            tokens.append(tok_sen)\n",
    "            tokens_lemma.append(process_lemma(lemmatizer,tok_sen))\n",
    "        data_21.at[index, 'tokenized_words_lemmatize'] = tokens_lemma\n",
    "\n",
    "\n",
    "    for index, row in tqdm(data_21.iterrows(), total = len(data_21)):\n",
    "        tokens = []\n",
    "        for sentence in row['tokenized_words_lemmatize']:\n",
    "            sen = []\n",
    "            for word in sentence:\n",
    "                if word.isalnum():\n",
    "                    tokens.append(word)\n",
    "            \n",
    "        data_21.at[index,'tokenized_words_lemmatize']  = tokens\n",
    "\n",
    "\n",
    "    unique_pmids = data_21['PMID'].drop_duplicates()\n",
    "\n",
    "    results_path = os.path.join(os.path.join(results_folder,'data_processed_whole.csv'))\n",
    "    data_21.to_csv(results_path, index=False)\n",
    "\n",
    "    return results_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_lda(data_path, models_path, results_path, timestamp, num_topics = 10):\n",
    "    from gensim import corpora, models\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \"\"\"Performs lda keywords extraction for data after lemmatization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to preprocessed dataset. Dataset must contain a column with name 'tokenized_words_lemmatize'.\n",
    "    \n",
    "    models_path : str\n",
    "        Path to save the model to (folder must exist).\n",
    "\n",
    "    results_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    timestamp : str\n",
    "        Timestamp that will be added to filenames\n",
    "\n",
    "    num_topic : int\n",
    "        Number of disired topics\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    (result_path, model_save_name) : tuple\n",
    "        Frist element is the path to created file with extracted keywrods, second - path to created model.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_topic_distribution(lda_model):\n",
    "        topics_distrib = {}\n",
    "        for t in lda_model.show_topics(21):\n",
    "            topics_distrib[t[0]] =[(a.split('*')[1][1:-1],float(a.split(\"*\")[0])) for a in t[1].split(' + ')]\n",
    "        return topics_distrib\n",
    "\n",
    "\n",
    "    train_data = pd.read_csv(data_path)\n",
    "    columns = ['tokenized_sentences', 'tokenized_words_lemmatize']\n",
    "    for col in columns:\n",
    "        train_data[col] = train_data[col].apply(eval)\n",
    "\n",
    "    texts = train_data.groupby('PMID')['tokenized_words_lemmatize'].agg(lambda x: x.iloc[0]+x.iloc[1])\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    \n",
    "    lda_model = models.LdaMulticore(corpus=corpus,\n",
    "                                        id2word=dictionary,\n",
    "                                        num_topics=num_topics,\n",
    "                                        passes = 20)\n",
    "    doc_lda = lda_model[corpus]\n",
    "\n",
    "    topic_distribution = get_topic_distribution(lda_model)\n",
    "    topics_results = pd.DataFrame.from_records([topic_distribution]).T.reset_index().rename(columns = {'index':'topic_number',0:'topic_keywords'})\n",
    "    topics_results.to_csv(os.path.join(results_path, f'topic_distribution_LDA_{timestamp}.csv'))\n",
    "    \n",
    "\n",
    "    docs_train= []\n",
    "    for doc in doc_lda:\n",
    "        docs_train.append({\n",
    "            'topic_number':doc[0][0],\n",
    "            'topic_probs': float(doc[0][1]),\n",
    "            'topic_keywords': topics_results.iloc[doc[0][0]]['topic_keywords']\n",
    "\n",
    "        })\n",
    "    docs_train = pd.DataFrame.from_records(docs_train)\n",
    "\n",
    "    train_results = train_data[['PMID']].drop_duplicates().reset_index(drop=True).join(docs_train)\n",
    "    results_path = os.path.join(os.path.join(results_path, f'LDA_{timestamp}.csv'))\n",
    "    train_results.to_csv(results_path)\n",
    "\n",
    "    models_path = os.path.join(models_path,f\"lda_model_{timestamp}\")\n",
    "    lda_model.save(models_path)\n",
    "    return results_path,models_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_disambiguation(results_folder,data_path, tagger_path, embedings_path,timestamp,with_21 = False,weigthing=False,sorting=False):\n",
    "    \"\"\"Performs disambiguation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_folder : str\n",
    "        Path to save results\n",
    "    \n",
    "    data_path : str\n",
    "        Path to save the model to (folder must exist).\n",
    "\n",
    "    tagger_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    embedings_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    timestamp : str\n",
    "        Timestamp that will be added to filenames\n",
    "\n",
    "    with_21: bool\n",
    "        Is column correspnding to 21 semantic types avaiable\n",
    "\n",
    "    weigthing: bool\n",
    "        Should the weigthed voting be performed\n",
    "\n",
    "    sorting: bool\n",
    "        Should the initial sorting be performed\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    result_path : str\n",
    "        Path to results\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    import pandas as pd\n",
    "    import os \n",
    "    import math\n",
    "    import numpy as np\n",
    "\n",
    "    def get_embeding(word,emb):\n",
    "        return emb.loc[word]\n",
    "    def create_tags_list_dict(row, with_sorting = False):\n",
    "        # dictionary keyword: list of concepts\n",
    "        # sorting enabled\n",
    "        result = {}\n",
    "        for tag in row:\n",
    "            key = tag[0].split(',')[0].upper()\n",
    "            value = tag[1].upper()\n",
    "            if key in result.keys():\n",
    "                if  value not in result[key]:\n",
    "                    result[key][value]=1\n",
    "                else:\n",
    "                    result[key][value]+=1\n",
    "            else:\n",
    "                result[key]= {value:1}\n",
    "        if with_sorting:\n",
    "            for key in result.keys():\n",
    "                d = result[key]\n",
    "                d = {k:0 for k,v in dict(sorted(d.items(), key=lambda item: item[1], reverse = True)).items()}\n",
    "                result[key] = d\n",
    "        else:\n",
    "            for key in result.keys():\n",
    "                d = result[key]\n",
    "                d = {k:0 for k,v in d.items()}\n",
    "                result[key] = d\n",
    "            \n",
    "\n",
    "        return result\n",
    "\n",
    "    def disambiguation(current_selection,embedings, weigths):\n",
    "        ''' \n",
    "        current_selection : dictionary keyword: list of all unique concepts\n",
    "        weigths: the importance of given keyword\n",
    "        \n",
    "        '''\n",
    "        # we iterate over the current_selection MAX_ITER times\n",
    "        vis = []\n",
    "        iterations = dict(zip(current_selection.keys(),[0]*len(current_selection)))\n",
    "        new_current_selction  = copy.deepcopy(current_selection)\n",
    "        should_stop = False\n",
    "        for i in range(7):\n",
    "\n",
    "            for keyword, concepts_list in new_current_selction.items():\n",
    "                if iterations[keyword]>0:\n",
    "                    break\n",
    "                distances = {} # for each possible concept calaculate the mean distance from other kewords (concepts of them)\n",
    "                for concept in concepts_list.keys():\n",
    "                    distances[concept] = []\n",
    "                    for k, current_best_tags in new_current_selction.items():\n",
    "                        # foreach keyword that is not a current one \n",
    "                        if k!=keyword:\n",
    "                            current_best_tag = list(current_best_tags.keys())[0] # the first out of list of concepts\n",
    "                            try:\n",
    "                                distances[concept].append(weigths[k]*math.dist(get_embeding(concept,embedings),get_embeding(current_best_tag,embedings))) # append distance from this concept\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                    distances[concept] = np.mean(distances[concept]) # mean distance \n",
    "                if keyword == 'COFFEE':\n",
    "                    vis.append((i,distances))\n",
    "                if list(new_current_selction[keyword].values()) == list(dict(sorted(distances.items(), key=lambda item: item[1])).values()):\n",
    "                    iterations[keyword] = i\n",
    "                new_current_selction[keyword] = dict(sorted(distances.items(), key=lambda item: item[1]))  # upadate the current selection of this keyword\n",
    "        return new_current_selction,vis, iterations\n",
    "        \n",
    "    def keywords_importance(grouped_data, tagger_data):\n",
    "        return grouped_data.reset_index().merge(tagger_data[['PMID','topic_keywords']] ,on = 'PMID').set_index('text_to_annotate')\n",
    "\n",
    "    def get_n_best_tags(data, n = 1):\n",
    "        return [{k:sorted(v, key=v.get)[:n] for k,v in dd.items()} for dd in data['after_disambiguation']]\n",
    "    def prepare_disambiguation(data, tagger, embedings,column_name = 'ncbo_annotations_pairs' ,  weighting = False, sorting = False, take_best = 1):\n",
    "        grouped = data.groupby('text_to_annotate').nth(0)\n",
    "        # get importance for each keyword -> will be used if weighting True\n",
    "        grouped = keywords_importance(grouped, tagger )\n",
    "        grouped['possible_tags'] = grouped[column_name].apply(lambda r: create_tags_list_dict(r, sorting))\n",
    "\n",
    "        # disambiguation\n",
    "        res = []\n",
    "        vis = []\n",
    "        its = []\n",
    "        for idx, row  in grouped.iterrows():\n",
    "            current_selection = row['possible_tags']\n",
    "            if not weighting:\n",
    "                weigths = dict(zip(list(row['topic_keywords'].keys()),[1] * len(row['topic_keywords'])))\n",
    "            else:\n",
    "                weigths = row['topic_keywords']\n",
    "            r,v, it= disambiguation(current_selection, embedings,weigths)\n",
    "            res.append(r)\n",
    "            vis.append(v)\n",
    "            its.append(it)\n",
    "        grouped['after_disambiguation'] = res\n",
    "        data = data.merge(grouped['after_disambiguation'].reset_index(), on = 'text_to_annotate' )\n",
    "        data['disambiguation_best_concept'] = get_n_best_tags(data, take_best)\n",
    "        return data,vis,its\n",
    "\n",
    "\n",
    "    def prepare_data(data_name, tagger_name, embedings_name,with_21 = True):\n",
    "        data = pd.read_csv(data_name)\n",
    "        data['ncbo_annotations_pairs'] = data['ncbo_annotations_pairs'].apply(eval)\n",
    "        data['ncbo_annotations_pairs']  = data['ncbo_annotations_pairs'].apply(lambda x : [[a[0].upper(),a[1]] for a in x])\n",
    "        if with_21:\n",
    "            data['ncbo_annotations_ST21pv_semtypes_pairs'] = data['ncbo_annotations_ST21pv_semtypes_pairs'].apply(eval)\n",
    "            data['ncbo_annotations_ST21pv_semtypes_pairs']  = data['ncbo_annotations_ST21pv_semtypes_pairs'].apply(lambda x : [[a[0].upper(),a[1]] for a in x])\n",
    "\n",
    "        tagger = pd.read_csv(tagger_name)\n",
    "        tagger['topic_keywords'] = tagger['topic_keywords'].apply(eval).apply(lambda x: {k.upper():v for k,v in dict(x).items()})\n",
    "\n",
    "\n",
    "        embedings = pd.read_csv(embedings_name)\n",
    "        embedings = embedings.set_index('words')\n",
    "        embedings.index = embedings.index.str.upper()\n",
    "        embedings = embedings[~embedings.index.duplicated(keep='first')]\n",
    "\n",
    "        return  data, tagger, embedings\n",
    "\n",
    "    def prepare_data(data_name, tagger_name, embedings_name,with_21 = True):\n",
    "        data = pd.read_csv(data_name)\n",
    "        data['ncbo_annotations_pairs'] = data['ncbo_annotations_pairs'].apply(eval)\n",
    "        data['ncbo_annotations_pairs']  = data['ncbo_annotations_pairs'].apply(lambda x : [[a[0].upper(),a[1]] for a in x])\n",
    "        if with_21:\n",
    "            data['ncbo_annotations_ST21pv_semtypes_pairs'] = data['ncbo_annotations_ST21pv_semtypes_pairs'].apply(eval)\n",
    "            data['ncbo_annotations_ST21pv_semtypes_pairs']  = data['ncbo_annotations_ST21pv_semtypes_pairs'].apply(lambda x : [[a[0].upper(),a[1]] for a in x])\n",
    "\n",
    "        tagger = pd.read_csv(tagger_name)\n",
    "        tagger['topic_keywords'] = tagger['topic_keywords'].apply(eval).apply(lambda x: {k.upper():v for k,v in dict(x).items()})\n",
    "\n",
    "\n",
    "        embedings = pd.read_csv(embedings_name)\n",
    "        embedings = embedings.set_index('words')\n",
    "        embedings.index = embedings.index.str.upper()\n",
    "        embedings = embedings[~embedings.index.duplicated(keep='first')]\n",
    "\n",
    "        return  data, tagger, embedings\n",
    "\n",
    "\n",
    "    data, tagger, embedings = prepare_data(data_path, tagger_path, embedings_path,with_21)\n",
    "\n",
    "    data_res, vis, its = prepare_disambiguation(data,tagger,embedings,'ncbo_annotations_pairs' )\n",
    "    results_path = os.path.join(results_folder,f'disambiguation_res_{timestamp}.csv')\n",
    "    data_res.to_csv(results_path)\n",
    "\n",
    "    return results_path\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/results\\\\disambiguation_res_2023-01-07_21-53-54.csv'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "data_folder = '../data'\n",
    "data_preprocessing_results = '../results_preprocessing'\n",
    "data_path = \"data_processed_whole.csv\"\n",
    "models_path = \"../models\"\n",
    "results_path = \"../results\"\n",
    "dataset = 'CRAFT'\n",
    "CRAFT_ONTOLOGIES = ['CHEBI', 'CL', 'GO', 'MONDO', 'MOP', 'NCBITAXON', 'PR', 'SO', 'UBERON']\n",
    "timestamp = get_now_str()\n",
    "\n",
    "prepared_data_path = prepare_data(data_folder,data_preprocessing_results,dataset)\n",
    "extracted_keywords_path, model_path = get_keywords_bertopic(prepared_data_path, models_path, results_path, timestamp)\n",
    "extracted_keywords_path, model_path = get_keywords_lda(os.path.join(data_preprocessing_results,data_path), models_path, results_path, timestamp)\n",
    "tagged_keywords_path = tag_ncbo(CRAFT_ONTOLOGIES, 'bertopic', extracted_keywords_path, results_path, timestamp)\n",
    "dismbiguation_results_path = prepare_disambiguation(results_path,tagged_keywords_path,extracted_keywords_path,\n",
    "'CRAFT/results/embedings/ncbo_embeddings.csv',timestamp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envNNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69b1adaa11ceff177c5ff5a0e22271ae8b2837f309a3421a8d0197a8c1aada63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
