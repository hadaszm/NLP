{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_now_str():\n",
    "    import datetime\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def transform_strings_to_arrays(df, col_names):\n",
    "    for col in col_names:\n",
    "        df[col] = df[col].apply(eval)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_bertopic(data_path, models_path, results_path, timestamp):\n",
    "    \"\"\"Performs bertopic keywords extraction for data after lemmatization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to preprocessed dataset. Dataset must contain a column with name 'tokenized_words_lemmatize'.\n",
    "    \n",
    "    models_path : str\n",
    "        Path to save the model to (folder must exist).\n",
    "\n",
    "    results_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    timestamp : str\n",
    "        timestamp that will be added to filenames\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    (result_path, model_save_name) : tuple[str]\n",
    "        Frist element is the path to created file with extracted keywrods, second - path to created model.\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    from bertopic import BERTopic\n",
    "    import os \n",
    "    import numpy as np\n",
    "\n",
    "    # basic BertTopic keyword extraction\n",
    "    def train_transform_save(train_data, model_save_name, min_topic_size=10):\n",
    "        \n",
    "        # train transform\n",
    "        topic_model = BERTopic(min_topic_size=min_topic_size)\n",
    "        topics, probs = topic_model.fit_transform(train_data.values)\n",
    "\n",
    "        # save model\n",
    "        topic_model.save(model_save_name)\n",
    "\n",
    "        return topic_model, topics, probs\n",
    "\n",
    "\n",
    "    def load_transform_save(data, model_save_name, results_path):\n",
    "\n",
    "        # load model\n",
    "        loaded_model = BERTopic.load(model_save_name)\n",
    "\n",
    "        # transform for data \n",
    "        samples_topics, samples_probs = loaded_model.transform(data.values)\n",
    "        res_df = pd.DataFrame({\n",
    "            'PMID': np.unique(data.index),\n",
    "            'topic_number': samples_topics,\n",
    "            'topic_probs': samples_probs,\n",
    "            \"topic_keywords\": [loaded_model.get_topic(topic_number) for topic_number in samples_topics]\n",
    "        })\n",
    "        res_df.to_csv(results_path, index=False)\n",
    "        return loaded_model, res_df\n",
    "\n",
    "    ##############################################################################################################################\n",
    "\n",
    "    full_data = transform_strings_to_arrays(pd.read_csv(data_path), col_names=['tokenized_words_lemmatize'])\n",
    "\n",
    "    data = full_data.groupby(by = ['PMID'])['tokenized_words_lemmatize'].agg(lambda x: ' '.join(x.values[0] + x.values[1]))\n",
    "\n",
    "    model_name = f'bertopic_keywords_{timestamp}'\n",
    "    model_save_name = os.path.join(models_path, model_name)\n",
    "    result_path = os.path.join(results_path, 'bertopic', f'{model_name}.csv')\n",
    "\n",
    "    topic_model, topics, probs = train_transform_save(data, model_save_name, min_topic_size=3)\n",
    "    _, res_df = load_transform_save(data, model_save_name, result_path)\n",
    "\n",
    "    return result_path, model_save_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_ncbo(ontologies, keywords_extractor_name, extracted_keywords_path, results_path, timestamp):\n",
    "    \"\"\"Performs NCBO tagging for keywords extracted with get_keywords_bertopic or get_keywords_lda functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ontologies : list[str]\n",
    "        List of string of ontologies ids that will be queried in tagging process.\n",
    "    \n",
    "    keywords_extractor_name : str\n",
    "        Name of the algorithm used to extract keywrods (for file/folders naming)\n",
    "\n",
    "    extracted_keywords_path : str\n",
    "        Path to the file returned by get_keywords_bertopic or get_keywords_lda functions.\n",
    "\n",
    "    results_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    timestamp : str\n",
    "        timestamp that will be added to filenames\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    save_name : str\n",
    "        Path to tagged words file.\n",
    "    \"\"\"\n",
    "\n",
    "    import urllib.request, urllib.error, urllib.parse\n",
    "    import json\n",
    "    import os\n",
    "    from pprint import pprint\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    REST_URL = \"http://data.bioontology.org\"\n",
    "    API_KEY = \"194c9635-ce67-4e70-81c5-898c3a2b30fb\"\n",
    "\n",
    "    def read_keywords_extraction_results(path):\n",
    "        data = pd.read_csv(path, index_col=0)\n",
    "        data = transform_strings_to_arrays(data, col_names = ['topic_keywords'])\n",
    "        data['text_to_annotate'] = data.topic_keywords.apply(\n",
    "            lambda row: re.sub(r\"[\\'\\[\\]]\", \"\", str([keyword[0] for keyword in row]))\n",
    "            )\n",
    "        return data\n",
    "\n",
    "    def get_json(url):\n",
    "        opener = urllib.request.build_opener()\n",
    "        opener.addheaders = [('Authorization', 'apikey token=' + API_KEY)]\n",
    "        return json.loads(opener.open(url).read())\n",
    "\n",
    "    def create_annotation_pairs(sample_row, column_name):\n",
    "        found_concepts = sample_row[column_name]\n",
    "        res_ann_pairs= []\n",
    "        for _, concept in enumerate(found_concepts):\n",
    "            max_trials = 5\n",
    "            trials_no = 0\n",
    "            while trials_no < max_trials:\n",
    "                try:\n",
    "                    concept_class = get_json(concept[\"annotatedClass\"][\"links\"][\"self\"])\n",
    "                    concept_class_ancestors = get_json(concept[\"annotatedClass\"]['links']['ancestors'])\n",
    "                    break\n",
    "                except:\n",
    "                    trials_no+=1\n",
    "                    continue\n",
    "            if trials_no==max_trials:\n",
    "                raise Exception(\"number of unsuccessfull connection attempts is max_trials\")\n",
    "            annotations = concept['annotations']\n",
    "            # annotations for this class\n",
    "            for annot in annotations:\n",
    "                res_ann_pairs.append([annot['text'], concept_class[\"prefLabel\"], 'DIRECT', concept[\"annotatedClass\"][\"links\"][\"self\"]])\n",
    "            # annotations for ancestors\n",
    "            for annot in annotations:\n",
    "                for ancestor in concept_class_ancestors:\n",
    "                    res_ann_pairs.append([annot['text'], ancestor[\"prefLabel\"], 'ANCESTOR', concept[\"annotatedClass\"]['links']['ancestors']])\n",
    "        unique_ann_pairs = [list(x) for x in set(tuple(x) for x in res_ann_pairs)]\n",
    "        return unique_ann_pairs\n",
    "\n",
    "        \n",
    "    ##########################################################################################################################\n",
    "\n",
    "    # read data\n",
    "    data = read_keywords_extraction_results(extracted_keywords_path)\n",
    "\n",
    "    # annotate data\n",
    "    data['ncbo_annotations'] \\\n",
    "        = data.text_to_annotate.apply(lambda text:  \\\n",
    "            get_json(REST_URL + f\"/annotator?ontologies={','.join(ontologies)}&text=\" + urllib.parse.quote(text)))\n",
    "\n",
    "    data = data.reset_index()[['PMID', 'text_to_annotate', 'ncbo_annotations']]\n",
    "\n",
    "    data_to_annotate = data[['text_to_annotate', 'ncbo_annotations']]\n",
    "    data_to_annotate = data_to_annotate.loc[data_to_annotate.astype(str).drop_duplicates().index]\n",
    "    data_to_annotate['ncbo_annotation_pairs'] = data_to_annotate.apply(create_annotation_pairs, column_name='ncbo_annotations', axis = 1)\n",
    "\n",
    "    # create annotation pairs\n",
    "    data_to_annotate[['text_to_annotate', 'ncbo_annotation_pairs']].to_dict()\n",
    "    text_to_annot_ncbo_pairs = dict(zip(data_to_annotate.text_to_annotate, data_to_annotate.ncbo_annotation_pairs))\n",
    "    data['ncbo_annotations_pairs'] = data['text_to_annotate'].apply(lambda text: text_to_annot_ncbo_pairs[text])\n",
    "\n",
    "    # save data\n",
    "    res_folder = f'{results_path}/{keywords_extractor_name}_ncbo'\n",
    "    if not os.path.exists(res_folder):\n",
    "        os.makedirs(res_folder)\n",
    "    save_name = f'{res_folder}/{keywords_extractor_name}_ncbo_{timestamp}.csv'\n",
    "    data.to_csv(save_name, index=False)\n",
    "\n",
    "    return save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/data_whole.csv\"\n",
    "models_path = \"../models\"\n",
    "results_path = \"../results\"\n",
    "CRAFT_ONTOLOGIES = ['CHEBI', 'CL', 'GO', 'MONDO', 'MOP', 'NCBITAXON', 'PR', 'SO', 'UBERON']\n",
    "timestamp = get_now_str()\n",
    "\n",
    "extracted_keywords_path, model_path = get_keywords_bertopic(data_path, models_path, results_path, timestamp)\n",
    "tagged_keywords_path = tag_ncbo(CRAFT_ONTOLOGIES, 'bertopic', extracted_keywords_path, results_path, timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88ba28277a1630fc5f744f385ba08b8a5477e64ab64787e3b7b12637e39579c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
